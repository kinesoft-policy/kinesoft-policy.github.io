<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="description" content="KineSoft: Learning Proprioceptive Manipulation Policies with Soft Robot Hands" />
  <!-- Social cards -->
  <meta property="og:title" content="KineSoft: Learning Proprioceptive Manipulation Policies with Soft Robot Hands" />
  <meta property="og:description" content="KineSoft leverages shape-based imitation and proprioceptive sensing to teach soft robot hands dexterous in-hand manipulation." />
  <meta property="og:image" content="static/images/kinesoft_thumb.png" />
  <meta property="og:url" content="https://kinesoft-policy.github.io" />
  <meta name="twitter:card" content="summary_large_image" />

  <title>KineSoft: Learning Proprioceptive Manipulation Policies with Soft Robot Hands</title>
  <link rel="icon" type="image/png" href="static/images/kinesoft_thumb.png" />

  <!-- Fonts & CSS util libs -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet" />
  <link rel="stylesheet" href="static/css/bulma.min.css" />
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css" />
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
  <link rel="stylesheet" href="static/css/index.css" />

  <!-- JS -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>
  <!-- HERO ---------------------------------------------------------->
  <section class="hero is-fullheight-with-navbar">
    <div class="hero-body">
      <div class="container is-max-desktop has-text-centered">
        <h1 class="title is-1 publication-title">KineSoft: Learning Proprioceptive Manipulation Policies with Soft Robot Hands</h1>

        <p class="is-size-5 publication-authors">
          <span class="author-block"><a href="https://uksangyoo.github.io" target="_blank">Uksang&nbsp;Yoo</a><sup>1</sup>,</span>
          <span class="author-block"><a href="https://jonfranc.com/" target="_blank">Jonathan&nbsp;Francis</a><sup>1,2</sup>,</span>
          <span class="author-block"><a href="https://www.cs.cmu.edu/~jeanoh/" target="_blank">Jean&nbsp;Oh</a><sup>1</sup>,</span>
          <span class="author-block"><a href="https://ichnow.ski" target="_blank">Jeffrey&nbsp;Ichnowski</a><sup>1</sup></span>
        </p>
        <p class="is-size-6">
          <sup>1</sup>Carnegie Mellon University &nbsp;|&nbsp;
          <sup>2</sup>Bosch Center for AI
        </p>
        <p class="is-size-6 has-text-grey">9th Conference on Robot Learning (CoRL 2025), Seoul, Korea</p>

        <!-- LINKS -->
        <div class="buttons is-centered mt-4">
          <a class="button is-rounded is-dark" href="static/pdfs/kinesoft.pdf" target="_blank">
            <span class="icon"><i class="fas fa-file-pdf"></i></span><span>Paper</span>
          </a>
          <a class="button is-rounded is-dark" href="static/pdfs/kinesoft_appendix.pdf" target="_blank">
            <span class="icon"><i class="fas fa-file-pdf"></i></span><span>Supplementary</span>
          </a>
          <a class="button is-rounded is-dark" href="https://arxiv.org/abs/2503.01078" target="_blank">
            <span class="icon"><i class="ai ai-arxiv"></i></span><span>arXiv</span>
          </a>
            <a class="button is-rounded is-dark" disabled>
            <span class="icon"><i class="fab fa-github"></i></span><span>Code (coming soon)</span>
            </a>
          <a class="button is-rounded is-dark" href="#citation">
            <span class="icon"><i class="fas fa-quote-right"></i></span><span>Cite</span>
          </a>
        </div>
      </div>
    </div>
  </section>

  <!-- OVERVIEW VIDEO ------------------------------------------------>
  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Overview</h2>
      <div class="video-container">
        <video autoplay muted loop controls>
          <source src="static/videos/overview.mp4" type="video/mp4" />
        </video>
      </div>
      <p class="subtitle has-text-centered mt-4">KineSoft combines shape-based imitation, proprioceptive sensing, and a shape-conditioned controller to achieve dexterous in-hand manipulation with compliant soft robot hands.</p>
    </div>
  </section>

  <!-- ABSTRACT ------------------------------------------------------>
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Abstract</h2>
      <div class="content is-medium has-text-justified">
        <p>
          Underactuated soft robot hands offer inherent safety and adaptability advantages over rigid systems. While imitation learning shows promise for acquiring complex dexterous manipulation skills, adapting existing methods to soft robots presents unique challenges in state representation and data collection. We propose <strong>KineSoft</strong>, a framework for direct kinesthetic teaching of soft robotic hands that leverages their natural compliance as a skill-teaching advantage rather than only as a control challenge. KineSoft makes three key contributions: (1) a shape-based imitation learning framework that uses proprioceptive feedback to ground diffusion-based policies, (2) a low-level shape-conditioned controller that enables precise tracking of desired shape trajectories, and (3) a sim-to-real learning approach for soft-robot mesh shape sensing with an internal strain-sensing array. In physical experiments, KineSoft significantly outperforms strain-signal baselines across six in-hand manipulation tasks involving both rigid and deformable objects.
        </p>
      </div>
    </div>
  </section>

  <!-- KEY CONTRIBUTIONS --------------------------------------------->
  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered mb-6">Key Contributions</h2>
      <div class="columns is-centered">
        <div class="column is-10">
          <div class="box key-finding">
            <h3 class="title is-4">üéØ Shape-Based Imitation Learning</h3>
            <p>KineSoft grounds diffusion-based policies in rich proprioceptive mesh representations, enabling efficient learning from kinesthetic demonstrations.</p>
          </div>
          <div class="box key-finding">
            <h3 class="title is-4">üéÆ Shape-Conditioned Low-Level Control</h3>
            <p>Our controller translates vertex-level shape errors into tendon commands, bridging the demonstration-to-execution gap typical in soft robots.</p>
          </div>
          <div class="box key-finding">
            <h3 class="title is-4">üîÑ Sim-to-Real Mesh Proprioception</h3>
            <p>A self-supervised domain-alignment procedure achieves &lt;2&nbsp;mm median shape error when transferring a simulation-trained estimator to real hardware.</p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- MOTIVATION VIDEO ----------------------------------------------->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Motivation</h2>
      <div class="video-container">
        <video autoplay muted loop controls>
          <source src="static/videos/motivation_1.mp4" type="video/mp4" />
        </video>
      </div>
      <p class="content has-text-centered mt-4">
        Soft robots' natural compliance enables intuitive kinesthetic teaching, where human demonstrators can directly manipulate the hand into desired configurations. KineSoft leverages this advantage for skill acquisition.
      </p>
    </div>
  </section>

  <!-- TECHNICAL APPROACH --------------------------------------------->
  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered mb-6">Technical Approach</h2>
      
      <!-- Shape Estimation -->
      <div class="columns is-vcentered mb-6">
        <div class="column is-6">
          <h3 class="title is-4">Shape Estimation</h3>
          <p>Our proprioceptive model uses internal strain sensors embedded in soft fingers to estimate real-time mesh deformations. A FoldingNet-based architecture predicts per-vertex displacements from sensor readings.</p>
        </div>
        <div class="column is-6">
          <div class="video-container">
            <video autoplay muted loop controls>
              <source src="static/videos/Shape_estimation.mp4" type="video/mp4" />
            </video>
          </div>
        </div>
      </div>

      <!-- Domain Alignment -->
      <div class="columns is-vcentered mb-6">
        <div class="column is-6">
          <div class="video-container">
            <video autoplay muted loop controls>
              <source src="static/videos/domain_alignment.mp4" type="video/mp4" />
            </video>
          </div>
        </div>
        <div class="column is-6">
          <h3 class="title is-4">Sim-to-Real Transfer</h3>
          <p>We use CMA-ES optimization to align simulated sensor models with real-world measurements, minimizing the Chamfer distance between observed and predicted point clouds. This enables robust transfer of simulation-trained models to physical hardware.</p>
        </div>
      </div>

      <!-- Low-Level Control -->
      <div class="columns is-vcentered">
        <div class="column is-6">
          <h3 class="title is-4">Shape-Conditioned Controller</h3>
          <p>The controller tracks desired shape trajectories by projecting vertex-level shape errors onto tendon actuation directions, enabling precise deformation control at 100 Hz.</p>
        </div>
        <div class="column is-6">
          <div class="video-container">
            <video autoplay muted loop controls>
              <source src="static/videos/low_level_control.mp4" type="video/mp4" />
            </video>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- TASKS / RESULTS ------------------------------------------------>
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered section-title">Experimental Tasks & Results</h2>
      
      <div class="content has-text-centered mb-5">
        <p class="is-size-5">We evaluated KineSoft on six challenging manipulation tasks:</p>
      </div>

      <!-- Task Grid -->
      <div class="columns is-multiline">
        <!-- Bottle Unscrewing -->
        <div class="column is-4">
          <div class="box">
            <h4 class="title is-5 has-text-centered">üçæ Bottle Unscrewing</h4>
            <div class="video-container">
              <video autoplay muted loop controls style="max-height: 200px;">
                <source src="static/videos/bottle_unscrewing.mp4" type="video/mp4" />
              </video>
            </div>
            <p class="has-text-centered mt-2">
              <strong>Success Rate: 85%</strong><br>
              <small>(Baseline: 0%)</small>
            </p>
          </div>
        </div>

        <!-- Container Unlidding -->
        <div class="column is-4">
          <div class="box">
            <h4 class="title is-5 has-text-centered">üì¶ Container Unlidding</h4>
            <div class="video-container">
              <video autoplay muted loop controls style="max-height: 200px;">
                <source src="static/videos/unlidding.mp4" type="video/mp4" />
              </video>
            </div>
            <p class="has-text-centered mt-2">
              <strong>Success Rate: 70%</strong><br>
              <small>(Baseline: 15%)</small>
            </p>
          </div>
        </div>

        <!-- Berry Picking -->
        <div class="column is-4">
          <div class="box">
            <h4 class="title is-5 has-text-centered">ü´ê Berry Picking</h4>
            <div class="video-container">
              <video autoplay muted loop controls style="max-height: 200px;">
                <source src="static/videos/blueberry.mp4" type="video/mp4" />
              </video>
            </div>
            <p class="has-text-centered mt-2">
              <strong>Success Rate: 80%</strong><br>
              <small>(Baseline: 35%)</small>
            </p>
          </div>
        </div>

        <!-- Lid Flicking -->
        <div class="column is-4">
          <div class="box">
            <h4 class="title is-5 has-text-centered">üîÑ Lid Flicking</h4>
            <div class="video-container">
              <video autoplay muted loop controls style="max-height: 200px;">
                <source src="static/videos/uncapping.mp4" type="video/mp4" />
              </video>
            </div>
            <p class="has-text-centered mt-2">
              <strong>Success Rate: 100%</strong><br>
              <small>(Baseline: 90%)</small>
            </p>
          </div>
        </div>

        <!-- Paper Grasping -->
        <div class="column is-4">
          <div class="box">
            <h4 class="title is-5 has-text-centered">üìÑ Paper Grasping</h4>
            <div class="video-container">
              <video autoplay muted loop controls style="max-height: 200px;">
                <source src="static/videos/paper.mp4" type="video/mp4" />
              </video>
            </div>
            <p class="has-text-centered mt-2">
              <strong>Success Rate: 95%</strong><br>
              <small>(Baseline: 65%)</small>
            </p>
          </div>
        </div>

        <!-- Fabric Grasping -->
        <div class="column is-4">
          <div class="box">
            <h4 class="title is-5 has-text-centered">üßµ Fabric Grasping</h4>
            <div class="video-container">
              <video autoplay muted loop controls style="max-height: 200px;">
                <source src="static/videos/fabric.mp4" type="video/mp4" />
              </video>
            </div>
            <p class="has-text-centered mt-2">
              <strong>Success Rate: 65%</strong><br>
              <small>(Baseline: 5%)</small>
            </p>
          </div>
        </div>
      </div>

      <!-- Results Summary -->
      <div class="box mt-5">
        <h3 class="title is-4 has-text-centered">üìä Key Results</h3>
        <div class="content">
          <ul>
            <li><strong>Shape Estimation:</strong> 1.92 mm error (41% improvement over best baseline)</li>
            <li><strong>Trajectory Tracking:</strong> 3.29 mm error (47% improvement over strain-tracking)</li>
            <li><strong>Task Performance:</strong> Average 82% success rate across all tasks</li>
          </ul>
        </div>
      </div>
    </div>
  </section>

  <!-- TECHNICAL DETAILS ---------------------------------------------->
  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered mb-6">System Components</h2>
      
      <!-- MOE Platform -->
      <div class="box">
        <h3 class="title is-4">ü§ñ MOE Soft Robot Platform</h3>
        <div class="columns is-vcentered">
          <div class="column is-8">
            <p>
              We use the Multifinger Omnidirectional End-effector (MOE) platform with embedded conductive rubber sensors. Each finger contains 4 strain sensors providing real-time proprioceptive feedback at 400 Hz.
            </p>
          </div>
          <div class="column is-4">
            <div class="video-container">
              <video autoplay muted loop controls>
                <source src="static/videos/Sensor.mp4" type="video/mp4" />
              </video>
            </div>
          </div>
        </div>
      </div>

      <!-- Simulation -->
      <div class="box">
        <h3 class="title is-4">üíª Simulation Framework</h3>
        <div class="columns is-vcentered">
          <div class="column is-4">
            <div class="video-container">
              <video autoplay muted loop controls>
                <source src="static/videos/Simulation.mp4" type="video/mp4" />
              </video>
            </div>
          </div>
          <div class="column is-8">
            <p>
              Training data generated using SOFA framework with Neo-Hookean hyperelastic material models. Random external forces simulate contact-rich interactions for robust shape estimation.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- CONCLUSIONS ---------------------------------------------------->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Conclusions & Impact</h2>
      <div class="content is-medium">
        <p>
          KineSoft demonstrates that soft robots' inherent compliance can be leveraged as an advantage for skill learning through kinesthetic teaching. Our shape-based hierarchical approach successfully bridges the gap between demonstration and execution modes, enabling effective dexterous manipulation.
        </p>
        <p>
          <strong>Key Insights:</strong>
        </p>
        <ul>
          <li>Shape representations provide consistent geometric grounding across demonstration and execution</li>
          <li>Domain alignment enables robust sim-to-real transfer without paired real-world labels</li>
          <li>Soft robots can achieve high success rates in contact-rich manipulation tasks</li>
        </ul>
        <p>
          This work opens new possibilities for intuitive programming of soft robotic systems in applications requiring safe, compliant interaction with delicate objects and humans.
        </p>
      </div>
    </div>
  </section>

  <!-- CITATION ------------------------------------------------------->
  <section id="citation" class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Citation</h2>
      <pre><code>@inproceedings{yoo2025kinesoft,
  title={KineSoft: Learning Proprioceptive Manipulation Policies with Soft Robot Hands},
  author={Yoo, Uksang and Francis, Jonathan and Oh, Jean and Ichnowski, Jeffrey},
  booktitle={Proceedings of the 9th Conference on Robot Learning (CoRL)},
  year={2025}
}</code></pre>
    </div>
  </section>

  <!-- ACKNOWLEDGMENTS ------------------------------------------------>
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Acknowledgments</h2>
      <div class="content has-text-centered">
        <p>
          We thank the reviewers for their valuable feedback and the CMU Robotics Institute for supporting this research.
        </p>
      </div>
    </div>
  </section>

  <!-- FOOTER --------------------------------------------------------->
  <footer class="footer">
    <div class="content has-text-centered">
      <p>
        Website template adapted from <a href="https://github.com/nerfies/nerfies.github.io" target="_blank">Nerfies</a> under the Apache&nbsp;2.0 license.
      </p>
    </div>
  </footer>

  <!-- Custom styles for better presentation -->
  <style>
    .video-container {
      position: relative;
      width: 100%;
      padding-bottom: 56.25%; /* 16:9 aspect ratio */
      height: 0;
      overflow: hidden;
    }
    
    .video-container video {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      object-fit: cover;
    }
    
    .box.key-finding {
      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
      color: white;
      transition: transform 0.3s;
    }
    
    .box.key-finding:hover {
      transform: translateY(-5px);
    }
    
    .box.key-finding h3 {
      color: white;
    }
    
    pre code {
      font-size: 0.85em;
    }
    
    .columns.is-vcentered {
      align-items: center;
    }
  </style>
</body>
</html>